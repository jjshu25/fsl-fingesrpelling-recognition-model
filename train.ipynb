{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6DBLRE-fqlO5"},"outputs":[],"source":["!pip install --upgrade pip\n","!pip install mediapipe-model-maker"]},{"cell_type":"markdown","metadata":{"id":"v3CvTNmB1WiY"},"source":["Import the required libraries."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"h8D23CqMymHQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c74UL9oI0VKU"},"outputs":[],"source":["from google.colab import files\n","import os\n","import tensorflow as tf\n","assert tf.__version__.startswith('2')\n","from mediapipe_model_maker import gesture_recognizer\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"i8fMLXTdD6tW"},"source":["### FSL dataset"]},{"cell_type":"markdown","source":["> Connect to google drive"],"metadata":{"id":"txMI97UZBpp3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6dwmyg5MnR_y"},"outputs":[],"source":["!unzip \"/content/drive/MyDrive/THESIS/DATASETALPHA4.zip\"\n","dataset_path = \"fsl_alphabet_train_1\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgadM4VDj3Y2"},"outputs":[],"source":["print(dataset_path)\n","labels = []\n","for i in os.listdir(dataset_path):\n","  if os.path.isdir(os.path.join(dataset_path, i)):\n","    labels.append(i)\n","print(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sx8PsrwYjvgO"},"outputs":[],"source":["NUM_EXAMPLES = 4\n","\n","for label in labels:\n","  label_dir = os.path.join(dataset_path, label)\n","  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n","  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n","  for i in range(NUM_EXAMPLES):\n","    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n","    axs[i].get_xaxis().set_visible(False)\n","    axs[i].get_yaxis().set_visible(False)\n","  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n","\n","plt.show()"]},{"cell_type":"markdown","source":["## Load the FSL Dataset"],"metadata":{"id":"i2U-V6fKBydA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTTNZsolKXiT"},"outputs":[],"source":["data = gesture_recognizer.Dataset.from_folder(\n","    dirname=dataset_path,\n","    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",")\n","train_data, rest_data = data.split(0.8)\n","validation_data, test_data = rest_data.split(0.5)"]},{"cell_type":"markdown","metadata":{"id":"yAXWc3bv8hpe"},"source":["## Train the loaded Dataset\n","\n","> Based on FSL with 300 pictures every fingerspelling."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yk0UiRB6NZrb"},"outputs":[],"source":["hparams = gesture_recognizer.HParams(\n","    export_dir=\"exported_model\",\n","    learning_rate=0.001,\n","    batch_size=32,\n","    epochs=50,\n","    lr_decay=0.95,\n","    gamma=2,\n",")\n","model_options = gesture_recognizer.ModelOptions(\n","    dropout_rate=0.3,\n","    layer_widths=[128, 64]\n",")\n","options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams, model_options=model_options\n",")\n","model_2 = gesture_recognizer.GestureRecognizer.create(\n","    train_data=train_data,\n","    validation_data=validation_data,\n","    options=options\n",")"]},{"cell_type":"markdown","metadata":{"id":"nED7mdIO9YS6"},"source":["## Evaluate the model performance\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdOqllqx9YKy"},"outputs":[],"source":["loss, acc = model_2.evaluate(test_data, batch_size=1)\n","print(f\"Test loss:{loss}, Test accuracy:{acc}\")"]},{"cell_type":"markdown","metadata":{"id":"vJLramjy9gvy"},"source":["## Export to Tensorflow Lite Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fmNaFXytijVg"},"outputs":[],"source":["model_2.export_model()\n","!ls exported_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yfN_47qjjOC"},"outputs":[],"source":["files.download('exported_model/gesture_recognizer.task')"]}],"metadata":{"colab":{"last_runtime":{"build_target":"","kind":"local"},"private_outputs":true,"provenance":[{"file_id":"https://github.com/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb","timestamp":1742914309838}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}